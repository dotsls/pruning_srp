{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c95aca3",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and environment setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch, timm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split, WeightedRandomSampler, DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "_ = torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afef2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common variables: device\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_available() else (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a61d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def cmp_arrays(a, b):\n",
    "    if torch._is_all_true(a == b):\n",
    "        print('IDENTICAL')\n",
    "        return True\n",
    "    else:\n",
    "        for ats, at in zip(['1e-8', '1e-6', '1e-4'],[1e-8, 1e-6, 1e-4]): \n",
    "            if torch.allclose(a, b, atol=at):\n",
    "                print(f'CLOSE ENOUGH (atol < {ats})')\n",
    "                return True\n",
    "        print(f'abs distance: {torch.dist(a, b, p=1).item()}')\n",
    "        return False\n",
    "    \n",
    "def test_correctness(A, B, compare_fn, args=[], kwargs={}):\n",
    "        a = A(*args, **kwargs)\n",
    "        b = B(*args, **kwargs)\n",
    "        compare_fn(a, b)\n",
    "\n",
    "def test_speed(A, B, n, args=[], kwargs={}):\n",
    "    for _ in tqdm(range(n[0])):\n",
    "        A(*args, **kwargs)\n",
    "    for _ in tqdm(range(n[1])):\n",
    "        B(*args, **kwargs)\n",
    "\n",
    "def get_random_sample(dataset):\n",
    "    i = np.random.randint(len(dataset))\n",
    "    return dataset[i][0].to(device).unsqueeze(0)\n",
    "\n",
    "def count_classes(ds, indices):\n",
    "    targets = ds.dataset.targets\n",
    "    classes = torch.tensor(targets)[indices]\n",
    "    counts = torch.zeros((100,))\n",
    "    for i in range(100):\n",
    "        counts[i] = (classes == i).sum()\n",
    "    return counts\n",
    "\n",
    "def get_balanced_indices(labels, subset_size):\n",
    "    \"\"\"\n",
    "        labels: list of labels of data samples\n",
    "        subset_size: factor by which to reduce dataset\n",
    "    \"\"\"\n",
    "    labels = torch.tensor(labels, dtype=torch.int32)\n",
    "    num_classes = torch.unique(labels).numel()\n",
    "\n",
    "    indices = [torch.nonzero(labels==i).flatten() for i in range(num_classes)]\n",
    "\n",
    "    numel = int(labels.numel() * subset_size)\n",
    "    per_class = torch.full((num_classes,), int(numel / num_classes), dtype=torch.int32)\n",
    "    remainder = int(numel - int(numel / num_classes)*num_classes)\n",
    "    per_class[:remainder] += 1\n",
    "\n",
    "    per_class_indices = []\n",
    "    for i in range(num_classes):\n",
    "        p = torch.randperm(indices[i].numel())\n",
    "        per_class_indices.append(indices[i][p[:per_class[i]]])\n",
    "\n",
    "    p = torch.randperm(numel)\n",
    "    balanced = list(per_class_indices[0])\n",
    "    for i in range(1, num_classes):\n",
    "        balanced = balanced + list(per_class_indices[i])\n",
    "    balanced = torch.tensor(balanced, dtype=torch.int32)[p]\n",
    "    return list(balanced)\n",
    "\n",
    "def test_model(model, dataloader):\n",
    "    sm = torch.nn.Softmax(dim=1)\n",
    "    with torch.no_grad():\n",
    "        acc, correct = 0, 0\n",
    "        for features, labels in tqdm(iter(dataloader)):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            clf = sm(model(features)).argmax(1)\n",
    "            correct += (clf == labels).sum()\n",
    "    acc = correct / len(dataloader.dataset)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8de79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_model_timm, load_dataset\n",
    "def load_model_timm(model_type, dataset_name, verbose=False):\n",
    "    \"\"\" \n",
    "    model   types: B/16, S/16 or Ti/16\n",
    "    dataset names: cifar100 or oxford-iiit-pet\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    index = pd.read_csv('models/index.csv')\n",
    "    pretrains = set(\n",
    "        index.query('ds==\"i21k\"').groupby('name').apply(\n",
    "        lambda df: df.sort_values('final_val').iloc[-1], \n",
    "        include_groups=False).filename\n",
    "    )\n",
    "    finetunes = index.loc[index.filename.apply(lambda name: name in pretrains)]\n",
    "    checkpoint = (\n",
    "        finetunes.query(f'name==\"{model_type}\" and adapt_ds==\"{dataset_name}\"')\n",
    "        .sort_values('adapt_final_val').iloc[-1].adapt_filename\n",
    "    ) # Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--cifar100-steps_10k-lr_0.003-res_224\n",
    "    if verbose: print(f\"Loaded checkpoint: {checkpoint}\")\n",
    "    \n",
    "    timm_modelnames = {\n",
    "        'Ti/16-224': 'vit_tiny_patch16_224',\n",
    "        'Ti/16-384': 'vit_tiny_patch16_384',\n",
    "        'S/16-224': 'vit_small_patch16_224',\n",
    "        'S/16-384': 'vit_small_patch16_384',\n",
    "        'B/16-224': 'vit_base_patch16_224',\n",
    "        'B/16-384': 'vit_base_patch16_384'\n",
    "    }\n",
    "    num_classes = 100 if dataset_name == 'cifar100' else 37\n",
    "    res = int(checkpoint.split('_')[-1])\n",
    "    model = timm.create_model(timm_modelnames[f'{model_type}-{res}'], num_classes=num_classes)\n",
    "    \n",
    "    # downloading a checkpoint automatically\n",
    "    # may show an error, but still downloads the checkpoint\n",
    "    from tensorflow.io import gfile # type: ignore\n",
    "    if not gfile.exists(f'models/{checkpoint}.npz'):     \n",
    "        gfile.copy(f'gs://vit_models/augreg/{checkpoint}.npz', f'models/{checkpoint}.npz')\n",
    "    timm.models.load_checkpoint(model, f'models/{checkpoint}.npz')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad_(False)\n",
    "    return model\n",
    "\n",
    "def load_dataset(dataset_name, batch_size, model_cfg=None, subset_size = 1., res = (224, 224), train=False, download_dataset=False, do_balanced_subset=False):\n",
    "    \"\"\"\n",
    "    dataset name: cifar100 or oxford-iiit-pet\n",
    "    \"\"\"\n",
    "    dataset = (\n",
    "        datasets.CIFAR100('data/', train=train, download=download_dataset) if dataset_name == 'cifar100' \n",
    "        else datasets.OxfordIIITPet('data/', split=('trainval' if train else 'test'))\n",
    "    ) \n",
    "    if model_cfg is None:\n",
    "        m, s = [0.5]*3, [0.5]*3\n",
    "    else:\n",
    "        m, s = model_cfg['mean'], model_cfg['std']\n",
    "        res = model_cfg['input_size'][-2:]\n",
    "    dataset.transform = v2.Compose([\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=m, std=s),\n",
    "            v2.Resize(res),\n",
    "    ])\n",
    "    if subset_size < 1.0:\n",
    "        n = len(dataset)\n",
    "        n_small = int(subset_size * n)\n",
    "        dataset, _ = random_split(dataset, [n_small, n - n_small])\n",
    "        if do_balanced_subset:\n",
    "            balanced_indices = get_balanced_indices(dataset.dataset.targets, subset_size)\n",
    "            dataset.indices = balanced_indices\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd627ea",
   "metadata": {},
   "source": [
    "## work-in-progress code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f929e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importance scoring functions\n",
    "def svd_score_reference(q, k):\n",
    "    \"\"\"q, k shape (batch, sequence, head, dimension)\"\"\"\n",
    "    U, S, Vt = torch.linalg.svd((q @ k.mT).cpu().double())\n",
    "    U, S, Vt = U.to(ref := q), S.to(ref), Vt.to(ref)\n",
    "    \n",
    "    U  =  U.unsqueeze(2).transpose(2, -1)\n",
    "    Vt = Vt.unsqueeze(2).transpose(2, -2)\n",
    "    S  =  S[..., None, None]\n",
    "    svd_maps = ((U * S) @ Vt).flatten(start_dim=-2)\n",
    "\n",
    "    q = q.unsqueeze(2).transpose(2, -1)\n",
    "    k = k.unsqueeze(2).permute(0, 1, 4, 2, 3)\n",
    "    att_maps = (q @ k).flatten(start_dim=-2)\n",
    "\n",
    "    torch.divide(att_maps, att_maps.norm(dim=-1, keepdim=True), out=att_maps)\n",
    "    torch.divide(svd_maps, svd_maps.norm(dim=-1, keepdim=True), out=svd_maps)\n",
    "    scores = torch.einsum('bhdm,bhnm->bhd', att_maps, svd_maps)\n",
    "\n",
    "    return scores\n",
    "\n",
    "def svd_score(q, k):\n",
    "    \"\"\"q, k shape (batch, sequence, head, dimensions)\"\"\"\n",
    "    _, RQ = torch.linalg.qr(q.cpu(), mode='r')\n",
    "    _, RK = torch.linalg.qr(k.cpu(), mode='r')\n",
    "    Ug, _, Vhg = torch.linalg.svd(RQ @ RK.mT, full_matrices=False)\n",
    "    RQ, RK = RQ.to(device), RK.to(device)\n",
    "    Uhg, Vhg = Ug.mT.to(device), Vhg.to(device)\n",
    "\n",
    "    F.normalize(RQ, dim=2, out=RQ)\n",
    "    F.normalize(RK, dim=2, out=RK)\n",
    "    torch.matmul(Uhg, RQ, out=RQ)\n",
    "    torch.matmul(Vhg, RK, out=RK)\n",
    "    scores = torch.linalg.vecdot(RQ, RK)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def similarity_score_reference(x):\n",
    "    B, _, D = x.shape\n",
    "    score = torch.ones((B, D)) * D\n",
    "    F.normalize(x, dim=1, out=x)\n",
    "    for (d1, d2) in np.ndindex(D, D):\n",
    "        score[:, d1] -= abs((x[:, :, d1] * x[:, :, d2]).sum(dim=1))\n",
    "    return score\n",
    "\n",
    "def similarity_score(x):\n",
    "    \"\"\"input shape (B, N, D) - batch, sequence len, embedding dimension\"\"\"\n",
    "    x = F.normalize(x, dim=1)\n",
    "    score = torch.einsum('bnd,bnt->bdt', x, x)\n",
    "    torch.abs(score, out=score)\n",
    "    score = x.shape[-1] - score.sum(dim=-1) \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d7fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention block with split qk & v matrices, convert_attention_blocks\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, attn):\n",
    "        super().__init__()\n",
    "        self.fused_attn = attn.fused_attn\n",
    "        \n",
    "        self.num_heads = attn.num_heads\n",
    "        self.scale = attn.scale\n",
    "        # qkv.weight.shape: (num_heads * (2*qk_dim + v_dim), emb_dim)\n",
    "        self.qkv = attn.qkv\n",
    "        self.q_norm = attn.q_norm\n",
    "        self.k_norm = attn.k_norm\n",
    "        self.attn_drop = attn.attn_drop\n",
    "        self.proj = attn.proj\n",
    "        self.proj_drop = attn.proj_drop\n",
    "        \n",
    "        # previous attention class assumed \n",
    "        # that all head dims for Q, K, V are the same\n",
    "        # between heads and between Q, K, V\n",
    "        # this attention class doesn't assume\n",
    "        # that head dims are the same between Q, K, V\n",
    "        # but, still, they're the same between heads of Q, K, V\n",
    "        self.qk_dim, self.v_dim = attn.head_dim, attn.head_dim\n",
    "        self.split_dim = [self.num_heads*c for c in [self.qk_dim]*2+[self.v_dim]]\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, _ = x.shape\n",
    "        q, k, v = torch.split(self.qkv(x), self.split_dim, dim=-1)\n",
    "        q = q.reshape(B, N, self.num_heads, self.qk_dim).transpose(1, 2)\n",
    "        k = k.reshape(B, N, self.num_heads, self.qk_dim).transpose(1, 2)\n",
    "        v = v.reshape(B, N, self.num_heads, self.v_dim).transpose(1, 2)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, self.split_dim[-1])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "def convert_attention_blocks(model):\n",
    "    for i, b in enumerate(model.blocks):\n",
    "        model.blocks[i].attn = Attention(b.attn)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80bcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hooks and hooks management\n",
    "class SNP:\n",
    "    def __init__(self, attn, do_svd_scoring=True, do_sim_scoring=True):\n",
    "        self.num_heads = attn.num_heads\n",
    "        self.qk_dim = attn.qk_dim\n",
    "        self.v_dim = attn.v_dim\n",
    "        self.split_dim = attn.split_dim\n",
    "        self.scale = attn.scale\n",
    "        self.do_svd_scoring = do_svd_scoring\n",
    "        self.do_sim_scoring = do_sim_scoring\n",
    "        self.svd_scores = 0\n",
    "        self.sim_scores = 0\n",
    "\n",
    "    def __call__(self, module, inp, out):\n",
    "        B, N = out.shape[:2]\n",
    "        q, k, v = torch.split(out, self.split_dim, dim=-1)\n",
    "        q = q.reshape(B, N, self.num_heads, self.qk_dim).transpose(1, 2)\n",
    "        k = k.reshape(B, N, self.num_heads, self.qk_dim).transpose(1, 2)\n",
    "        if self.do_svd_scoring:\n",
    "            self.svd_scores = svd_score(q, k)\n",
    "        if self.do_sim_scoring:\n",
    "            self.sim_scores = similarity_score(v).reshape(B, self.num_heads, self.v_dim)\n",
    "\n",
    "class MLP_SNP:\n",
    "    def __init__(self, module, do_scoring=True):\n",
    "        self.out_features = module.out_features\n",
    "        self.do_scoring = do_scoring\n",
    "        self.sim_scores = 0\n",
    "        \n",
    "    def __call__(self, module, inp, out):\n",
    "        if self.do_scoring:\n",
    "            self.sim_scores = similarity_score(out)\n",
    "\n",
    "def register_hooks(model, scoring_flags):\n",
    "    \"\"\"\n",
    "        scoring_flags: (SVD [QK], cos-sim [V], cos-sim [MLP])\n",
    "    \"\"\"\n",
    "    for b in model.blocks:\n",
    "        snp = SNP(b.attn, *scoring_flags[:2])\n",
    "        b.attn.qkv.register_forward_hook(snp)\n",
    "        mlp = MLP_SNP(b.mlp.fc1, scoring_flags[2])\n",
    "        b.mlp.fc1.register_forward_hook(mlp)\n",
    "    return model\n",
    "\n",
    "def remove_hooks(model):\n",
    "    for b in model.blocks:\n",
    "        b.attn.qkv._forward_hooks.popitem()\n",
    "        b.mlp.fc1._forward_hooks.popitem()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d971e66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect_scores, zero_out (weights)\n",
    "def collect_scores(model, dl):\n",
    "    svd_scores, sim_scores, mlp_scores = [], [], []\n",
    "    snp_hooks, mlp_hooks = [], []\n",
    "    for b in model.blocks:\n",
    "        snp_hooks.append(next(iter(b.attn.qkv._forward_hooks.values())))\n",
    "        mlp_hooks.append(next(iter(b.mlp.fc1._forward_hooks.values())))\n",
    "        hook = snp_hooks[-1]\n",
    "        svd_scores.append(torch.zeros((hook.num_heads, hook.qk_dim), device=device))\n",
    "        sim_scores.append(torch.zeros((hook.num_heads, hook.v_dim), device=device))\n",
    "        mlp_scores.append(torch.zeros((mlp_hooks[-1].out_features,), device=device))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, _ in tqdm(dl):\n",
    "            model(features.to(device))\n",
    "            for i, (h1, h2) in enumerate(zip(snp_hooks, mlp_hooks)):\n",
    "                if h1.do_svd_scoring: svd_scores[i] += h1.svd_scores.abs().sum(0)\n",
    "                if h1.do_sim_scoring: sim_scores[i] += h1.sim_scores.sum(0)\n",
    "                if h2.do_scoring: mlp_scores[i] += h2.sim_scores.sum(0)\n",
    "\n",
    "    return svd_scores, sim_scores, mlp_scores\n",
    "\n",
    "def zero_out(model, sparsity, svd_scores, sim_scores, mlp_scores):\n",
    "    svd_list, sim_list, mlp_list = [], [], []\n",
    "    for svd, sim, mlp in zip(svd_scores, sim_scores, mlp_scores):\n",
    "        svd_list.append(svd.sort().indices)\n",
    "        sim_list.append(sim.sort().indices)\n",
    "        mlp_list.append(mlp.sort().indices)\n",
    "\n",
    "    for b, svd, sim, mlp in zip(model.blocks, svd_list, sim_list, mlp_list):\n",
    "        m = b.attn\n",
    "        o = m.qk_dim*m.num_heads\n",
    "        ta, tb = int(m.qk_dim*sparsity[0]), int(m.v_dim*sparsity[1])\n",
    "        for h in range(m.num_heads):\n",
    "            ho, hvo = h * m.qk_dim, h * m.v_dim\n",
    "            \n",
    "            m.qkv.weight[svd[h, :ta]+ho] = 0\n",
    "            m.qkv.weight[svd[h, :ta]+ho+o] = 0\n",
    "\n",
    "            m.qkv.weight[sim[h, :tb]+2*o+hvo] = 0\n",
    "            m.proj.weight[:, sim[h, :tb]+hvo] = 0\n",
    "        \n",
    "        tc = int(b.mlp.fc1.out_features*sparsity[2])\n",
    "        b.mlp.fc1.weight[mlp[:tc]] = 0\n",
    "        b.mlp.fc2.weight[:, mlp[:tc]] = 0\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e002bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning testing function\n",
    "def test_pruning(scoring, testing, modelname, testlen, prunelen, sparsity_levels):\n",
    "    \"\"\"\n",
    "        scoring: boolean flags (SVD [QK], cos-sim [V], cos-sim [MLP])\n",
    "        testing: boolean flags (Att ,QK, V, MLP, All)\n",
    "        model_type   : ('B/16', 'S/16', 'Ti/16')\n",
    "        testlen, prunelen: float in [0, 1], size of dataset\n",
    "        sparsity_levels: list. which sparsity values to test\n",
    "    \"\"\"\n",
    "    # collecting importance scores\n",
    "    dl = load_dataset('cifar100', 4, subset_size=prunelen)[1]\n",
    "    model = convert_attention_blocks(load_model_timm(modelname, 'cifar100'))\n",
    "    register_hooks(model, scoring)\n",
    "    iscores = collect_scores(model, dl)\n",
    "    remove_hooks(model)\n",
    "\n",
    "    # variables\n",
    "    colors = ('tab:green', 'tab:red', 'tab:blue', 'tab:purple', 'tab:brown')\n",
    "    testing_flagnames = ('Att', 'SVD', 'Sim', 'MLP', 'All')\n",
    "    n_steps = len(sparsity_levels)\n",
    "    results = np.full((5, n_steps), float(test_model(model, dl))*100)\n",
    "    sl, z, old_opts = sparsity_levels, [0]*n_steps, np.get_printoptions()\n",
    "    sparsities = [list(zip(*t)) for t in ((sl,sl,z), (sl,z,z), (z,sl,z), (z,z,sl), (sl,sl,sl))]\n",
    "    dl  = load_dataset('cifar100', 8, subset_size=testlen )[1]\n",
    "\n",
    "    # evaluating performance after pruning\n",
    "    np.set_printoptions(precision=1, suppress=True, linewidth=100)\n",
    "    for stage in np.flatnonzero(testing):\n",
    "        for step in range(n_steps):\n",
    "            zero_out((m := deepcopy(model)), sparsities[stage][step], *iscores)\n",
    "            results[stage, step] = float(test_model(m, dl)) * 100\n",
    "\n",
    "            clear_output(); print(f'Sparsity   : {sparsity_levels*100}')\n",
    "            for k, (f, n, r) in enumerate(zip(testing, testing_flagnames, results)):\n",
    "                limit = n_steps if stage>k else step+1\n",
    "                if f and stage>=k: print(f'{n} pruning: {r[:limit]}')\n",
    "    np.set_printoptions(**old_opts)\n",
    "\n",
    "    # plotting results\n",
    "    for stage in np.flatnonzero(testing):\n",
    "        plt.plot(sl, results[stage], color=colors[stage], label=testing_flagnames[stage])\n",
    "    plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3c768",
   "metadata": {},
   "source": [
    "## test runs & experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a107f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning testing setup\n",
    "test_pruning(\n",
    "    [True, True, True], \n",
    "    [True, False, False, True, False], \n",
    "    'B/16', .04, .08,\n",
    "    np.linspace(0, 56/64, 15)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1dfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous pruning run results\n",
    "\n",
    "# results from pruning first n rows (essentially random)\n",
    "# Sparsity:      0.0,  6.2, 12.5, 18.8, 25.0, 31.2, 37.5, 43.8, 50.0, 56.2, 62.5, 68.8, 75.0, 81.2, 87.5, 93.8\n",
    "# Att pruning:  89.7, 78.0, 53.9, 25.2,  8.9,  5.2,  2.8,  2.2,  2.2,  2.2,  2.1,  2.2,  1.2,  1.4,  1.4,  1.4\n",
    "# SVD pruning:  89.7, 88.2, 84.6, 76.5, 70.1, 56.5, 38.2, 29.3, 19.6,  7.4,  2.6,  1.1,  0.7,  1.6,  1.5,  1.6\n",
    "# Sim pruning:  89.7, 81.8, 68.2, 50.1, 19.9, 10.9,  5.9,  3.6,  3.3,  3.1,  3.2,  2.2,  2.0,  1.2,  1.1,  1.2\n",
    "# MLP pruning:  89.7, 73.6, 37.6, 20.9,  9.4,  4.8,  3.0,  1.7,  1.7,  1.6,  1.5,  0.7,  0.6,  0.7,  1.0,  0.9\n",
    "# All pruning:  89.7, 57.6, 14.1,  5.3,  2.9,  1.0,  1.9,  1.0,  0.9,  1.7,  1.2,  1.1,  1.1,  1.0,  0.8,  0.4\n",
    "\n",
    "# before implementing stratified random subsampling\n",
    "# Sparsity:      0.0,  6.2, 12.5, 18.8, 25.0, 31.2, 37.5, 43.8, 50.0, 56.2, 62.5, 68.8, 75.0, 81.2, 87.5\n",
    "# Att pruning:  88.5,  7.6,  1.7,  1.1,  0.8,  1.2,  1.3,  1.2,  1.3,  1.7,  1.7,  1.0,  1.5,  1.2,  1.5\n",
    "# SVD pruning:  88.5, 86.6,  9.2,  6.5,  5.1,  3.3,  3.2,  3.7,  4.6,  5.0,  5.2,  4.5,  2.9,  1.9,  1.2\n",
    "# Sim pruning:  88.5,  8.5,  2.3,  3.0,  3.0,  3.2,  2.3,  2.5,  1.8,  2.7,  2.4,  1.0,  2.2,  1.0,  1.5\n",
    "# MLP pruning:  88.5, 76.1, 62.8, 39.6, 12.3, 13.7,  8.6,  7.4,  6.3,  5.4,  4.7,  2.7,  3.4,  2.2,  2.4\n",
    "# All pruning:  88.5,  4.6,  3.0,  1.3,  1.5,  2.0,  1.4,  1.7,  2.0,  1.3,  1.2,  1.0,  1.4,  1.0,  1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e04c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy test of timm model on subset of cifar100\n",
    "\n",
    "# print(test_model(\n",
    "#     convert_attention_blocks(load_model_timm('Ti/16', 'cifar100')),\n",
    "#     load_dataset('cifar100', 8, subset_size=.25)[1]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96282d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test correctness and speed of svd importance scoring algorithm\n",
    "\n",
    "# test_correctness(\n",
    "#     svd_score_reference, svd_score, cmp_arrays,\n",
    "#     (torch.randn(4, 3, 197, 64, device=device), torch.randn(4, 3, 197, 64, device=device))\n",
    "# )\n",
    "\n",
    "# test_speed(\n",
    "#     svd_score_reference, svd_score, [50, 500], \n",
    "#     (torch.randn(4, 3, 197, 64, device=device), torch.randn(4, 3, 197, 64, device=device))\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2750f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test correctness and speed of column similarity importance scoring\n",
    "\n",
    "# test_correctness(similarity_score_reference, similarity_score, cmp_arrays, [torch.randn((16,16,16))])\n",
    "\n",
    "# test_speed(similarity_score_reference, similarity_score, [4, 8000], args=[torch.randn(((4, 256, 256)))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
