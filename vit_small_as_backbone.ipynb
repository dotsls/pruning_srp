{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73abc2e0-4720-405c-9ee5-1add06179c82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:03:58.070770Z",
     "start_time": "2025-05-11T20:03:57.418685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/dino-vits16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Oxford Flowers 102 dataset...\n",
      "Training samples: 6551 (80%)\n",
      "Test samples: 1638 (20%)\n",
      "Total samples: 8189\n"
     ]
    }
   ],
   "source": [
    "# Experiment of using vit-small as backbone and using logistic regression as classification head\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from thop import profile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torchvision.datasets import Flowers102\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from transformers import ViTModel\n",
    "\n",
    "# Constants\n",
    "MODEL_NAME = \"facebook/dino-vits16\"  # ViT-Small from DINO\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 224\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Load Model\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = ViTModel.from_pretrained(MODEL_NAME).eval()\n",
    "\n",
    "# Standard normalization values for ImageNet pre-trained models\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Prepare Dataset\n",
    "transform = Compose([\n",
    "    Resize((IMAGE_SIZE, IMAGE_SIZE)),  # Resize to 224x224\n",
    "    ToTensor(),\n",
    "    Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "dataset_root = \"./data\" \n",
    "\n",
    "print(\"Loading Oxford Flowers 102 dataset...\")\n",
    "# Try to load all splits\n",
    "train_dataset = Flowers102(root=dataset_root, split=\"train\", transform=transform, download=True)\n",
    "val_dataset = Flowers102(root=dataset_root, split=\"val\", transform=transform, download=True)\n",
    "test_dataset = Flowers102(root=dataset_root, split=\"test\", transform=transform, download=True)\n",
    "    \n",
    "# Combine all splits to create a complete dataset\n",
    "complete_dataset = ConcatDataset([train_dataset, val_dataset, test_dataset])\n",
    "    \n",
    "# Calculate sizes for 80-20 split\n",
    "total_size = len(complete_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "test_size = total_size - train_size\n",
    "    \n",
    "# Create new random splits (80% train, 20% test)\n",
    "train_dataset, test_dataset = random_split(\n",
    "    complete_dataset,\n",
    "    [train_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(RANDOM_SEED)\n",
    ")\n",
    "   \n",
    "print(f\"Training samples: {len(train_dataset)} (80%)\")\n",
    "print(f\"Test samples: {len(test_dataset)} (20%)\")\n",
    "print(f\"Total samples: {total_size}\")\n",
    "    \n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99daf848f54e627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:07:07.951519Z",
     "start_time": "2025-05-11T20:03:59.351775Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features for the training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 205/205 [02:31<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 52/52 [00:37<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train features shape: (6551, 384)\n",
      "Test features shape: (1638, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "def extract_features(dataloader, model):\n",
    "    features = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # Disable gradients\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting features\"):\n",
    "            # Get images and labels\n",
    "            # Since we're using a ConcatDataset and random_split, \n",
    "            # we need to handle different dataset structures\n",
    "            if isinstance(batch, list):\n",
    "                # For custom datasets that return a list\n",
    "                images, targets = batch\n",
    "            elif hasattr(batch, 'data') and hasattr(batch, 'targets'):\n",
    "                # For some torchvision datasets\n",
    "                images, targets = batch.data, batch.targets\n",
    "            else:\n",
    "                # Default unpacking\n",
    "                try:\n",
    "                    images, targets = batch\n",
    "                except:\n",
    "                    print(f\"Unexpected batch format: {type(batch)}\")\n",
    "                    continue\n",
    "            \n",
    "            # Make sure targets is a tensor\n",
    "            if not isinstance(targets, torch.Tensor):\n",
    "                targets = torch.tensor(targets)\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            # Forward pass through the model\n",
    "            outputs = model(pixel_values=images)\n",
    "            # Extract [CLS] token embeddings (first token)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            # Store features and labels\n",
    "            features.append(embeddings.cpu().numpy())\n",
    "            labels.append(targets.cpu().numpy())\n",
    "\n",
    "        \n",
    "    return np.vstack(features), np.concatenate(labels)\n",
    "\n",
    "print(\"\\nExtracting features for the training set...\")\n",
    "train_features, train_labels = extract_features(train_loader, model)\n",
    "\n",
    "print(\"Extracting features for the test set...\")\n",
    "test_features, test_labels = extract_features(test_loader, model)\n",
    "    \n",
    "print(f\"Train features shape: {train_features.shape}\")\n",
    "print(f\"Test features shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cba7e637cd73a2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-11T20:08:36.000789Z",
     "start_time": "2025-05-11T20:08:31.904084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training logistic regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =        39270     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  4.62497D+00    |proj g|=  5.12441D-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " This problem is unconstrained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At iterate   50    f=  3.76031D-03    |proj g|=  1.74471D-04\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "39270     71     77      1     0     0   4.533D-05   3.104D-03\n",
      "  F =   3.1039340846413079E-003\n",
      "\n",
      "CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL            \n",
      "Accuracy on the test set: 97.99%\n",
      "Measuring latency...\n",
      "Latency (inference time for one sample): 0.63 ms\n",
      "Calculating FLOPs of ViT model...\n",
      "FLOPs (floating-point operations): 8,497,093,632.0\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression \n",
    "print(\"\\nTraining logistic regression...\")\n",
    "clf = LogisticRegression(max_iter=2000, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n",
    "clf.fit(train_features, train_labels)\n",
    "    \n",
    "# Evaluate Accuracy \n",
    "predictions = clf.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2%}\")\n",
    "    \n",
    "# Measure Latency\n",
    "print(\"Measuring latency...\")\n",
    "start_time = time.time()\n",
    "_ = clf.predict(test_features[:1])  # scikit-learn classifier latency\n",
    "end_time = time.time()\n",
    "latency = (end_time - start_time) * 1000\n",
    "print(f\"Latency (inference time for one sample): {latency:.2f} ms\")\n",
    "\n",
    "# Measure FLOPs of ViT \n",
    "print(\"Calculating FLOPs of ViT model...\")\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE).to(device)\n",
    "model = model.to(device)\n",
    "macs, params = profile(model, inputs=(dummy_input,), verbose=False)\n",
    "flops = macs * 2\n",
    "print(f\"FLOPs (floating-point operations): {flops:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3e08072fcaf032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
